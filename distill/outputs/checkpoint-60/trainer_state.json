{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.04759071980963712,
  "eval_steps": 500,
  "global_step": 60,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.000793178663493952,
      "grad_norm": 0.20480778813362122,
      "learning_rate": 0.0,
      "loss": 2.2929,
      "step": 1
    },
    {
      "epoch": 0.001586357326987904,
      "grad_norm": 0.20974531769752502,
      "learning_rate": 2e-05,
      "loss": 2.3256,
      "step": 2
    },
    {
      "epoch": 0.002379535990481856,
      "grad_norm": 0.19527533650398254,
      "learning_rate": 4e-05,
      "loss": 2.3056,
      "step": 3
    },
    {
      "epoch": 0.003172714653975808,
      "grad_norm": 0.21725331246852875,
      "learning_rate": 6e-05,
      "loss": 2.2732,
      "step": 4
    },
    {
      "epoch": 0.00396589331746976,
      "grad_norm": 0.20496508479118347,
      "learning_rate": 8e-05,
      "loss": 2.2651,
      "step": 5
    },
    {
      "epoch": 0.004759071980963712,
      "grad_norm": 0.18594321608543396,
      "learning_rate": 0.0001,
      "loss": 2.22,
      "step": 6
    },
    {
      "epoch": 0.005552250644457664,
      "grad_norm": 0.1979253739118576,
      "learning_rate": 9.818181818181818e-05,
      "loss": 2.2222,
      "step": 7
    },
    {
      "epoch": 0.006345429307951616,
      "grad_norm": 0.20056432485580444,
      "learning_rate": 9.636363636363637e-05,
      "loss": 2.2415,
      "step": 8
    },
    {
      "epoch": 0.007138607971445568,
      "grad_norm": 0.1811724752187729,
      "learning_rate": 9.454545454545455e-05,
      "loss": 2.2773,
      "step": 9
    },
    {
      "epoch": 0.00793178663493952,
      "grad_norm": 0.17779935896396637,
      "learning_rate": 9.272727272727273e-05,
      "loss": 2.1851,
      "step": 10
    },
    {
      "epoch": 0.008724965298433472,
      "grad_norm": 0.18098105490207672,
      "learning_rate": 9.090909090909092e-05,
      "loss": 2.2658,
      "step": 11
    },
    {
      "epoch": 0.009518143961927425,
      "grad_norm": 0.18328353762626648,
      "learning_rate": 8.90909090909091e-05,
      "loss": 2.4124,
      "step": 12
    },
    {
      "epoch": 0.010311322625421376,
      "grad_norm": 0.1608133167028427,
      "learning_rate": 8.727272727272727e-05,
      "loss": 2.2626,
      "step": 13
    },
    {
      "epoch": 0.011104501288915328,
      "grad_norm": 0.15923690795898438,
      "learning_rate": 8.545454545454545e-05,
      "loss": 2.3216,
      "step": 14
    },
    {
      "epoch": 0.01189767995240928,
      "grad_norm": 0.156307190656662,
      "learning_rate": 8.363636363636364e-05,
      "loss": 2.0508,
      "step": 15
    },
    {
      "epoch": 0.012690858615903232,
      "grad_norm": 0.16285920143127441,
      "learning_rate": 8.181818181818183e-05,
      "loss": 2.3645,
      "step": 16
    },
    {
      "epoch": 0.013484037279397185,
      "grad_norm": 0.16595126688480377,
      "learning_rate": 8e-05,
      "loss": 2.2295,
      "step": 17
    },
    {
      "epoch": 0.014277215942891136,
      "grad_norm": 0.1392715573310852,
      "learning_rate": 7.818181818181818e-05,
      "loss": 2.071,
      "step": 18
    },
    {
      "epoch": 0.015070394606385089,
      "grad_norm": 0.15096940100193024,
      "learning_rate": 7.636363636363637e-05,
      "loss": 2.1799,
      "step": 19
    },
    {
      "epoch": 0.01586357326987904,
      "grad_norm": 0.16781380772590637,
      "learning_rate": 7.454545454545455e-05,
      "loss": 2.195,
      "step": 20
    },
    {
      "epoch": 0.01665675193337299,
      "grad_norm": 0.16494829952716827,
      "learning_rate": 7.272727272727273e-05,
      "loss": 2.2167,
      "step": 21
    },
    {
      "epoch": 0.017449930596866944,
      "grad_norm": 0.1597335934638977,
      "learning_rate": 7.090909090909092e-05,
      "loss": 2.2542,
      "step": 22
    },
    {
      "epoch": 0.018243109260360896,
      "grad_norm": 0.15516316890716553,
      "learning_rate": 6.90909090909091e-05,
      "loss": 2.0833,
      "step": 23
    },
    {
      "epoch": 0.01903628792385485,
      "grad_norm": 0.16401024162769318,
      "learning_rate": 6.727272727272727e-05,
      "loss": 2.1159,
      "step": 24
    },
    {
      "epoch": 0.019829466587348802,
      "grad_norm": 0.17568230628967285,
      "learning_rate": 6.545454545454546e-05,
      "loss": 2.165,
      "step": 25
    },
    {
      "epoch": 0.02062264525084275,
      "grad_norm": 0.16692373156547546,
      "learning_rate": 6.363636363636364e-05,
      "loss": 2.1807,
      "step": 26
    },
    {
      "epoch": 0.021415823914336704,
      "grad_norm": 0.16346026957035065,
      "learning_rate": 6.181818181818182e-05,
      "loss": 2.2526,
      "step": 27
    },
    {
      "epoch": 0.022209002577830657,
      "grad_norm": 0.17018559575080872,
      "learning_rate": 6e-05,
      "loss": 2.1648,
      "step": 28
    },
    {
      "epoch": 0.02300218124132461,
      "grad_norm": 0.16726051270961761,
      "learning_rate": 5.818181818181818e-05,
      "loss": 2.3262,
      "step": 29
    },
    {
      "epoch": 0.02379535990481856,
      "grad_norm": 0.17689694464206696,
      "learning_rate": 5.636363636363636e-05,
      "loss": 2.32,
      "step": 30
    },
    {
      "epoch": 0.02458853856831251,
      "grad_norm": 0.17023351788520813,
      "learning_rate": 5.4545454545454546e-05,
      "loss": 2.2145,
      "step": 31
    },
    {
      "epoch": 0.025381717231806464,
      "grad_norm": 0.1815750002861023,
      "learning_rate": 5.272727272727272e-05,
      "loss": 2.221,
      "step": 32
    },
    {
      "epoch": 0.026174895895300417,
      "grad_norm": 0.18024279177188873,
      "learning_rate": 5.090909090909091e-05,
      "loss": 2.4216,
      "step": 33
    },
    {
      "epoch": 0.02696807455879437,
      "grad_norm": 0.17931412160396576,
      "learning_rate": 4.909090909090909e-05,
      "loss": 2.1769,
      "step": 34
    },
    {
      "epoch": 0.02776125322228832,
      "grad_norm": 0.16787974536418915,
      "learning_rate": 4.7272727272727275e-05,
      "loss": 2.2011,
      "step": 35
    },
    {
      "epoch": 0.028554431885782272,
      "grad_norm": 0.17119011282920837,
      "learning_rate": 4.545454545454546e-05,
      "loss": 2.2123,
      "step": 36
    },
    {
      "epoch": 0.029347610549276225,
      "grad_norm": 0.17189282178878784,
      "learning_rate": 4.3636363636363636e-05,
      "loss": 2.1824,
      "step": 37
    },
    {
      "epoch": 0.030140789212770178,
      "grad_norm": 0.16670148074626923,
      "learning_rate": 4.181818181818182e-05,
      "loss": 2.0148,
      "step": 38
    },
    {
      "epoch": 0.030933967876264127,
      "grad_norm": 0.1715080589056015,
      "learning_rate": 4e-05,
      "loss": 2.2909,
      "step": 39
    },
    {
      "epoch": 0.03172714653975808,
      "grad_norm": 0.1737983375787735,
      "learning_rate": 3.818181818181819e-05,
      "loss": 2.3802,
      "step": 40
    },
    {
      "epoch": 0.032520325203252036,
      "grad_norm": 0.15988215804100037,
      "learning_rate": 3.6363636363636364e-05,
      "loss": 2.1756,
      "step": 41
    },
    {
      "epoch": 0.03331350386674598,
      "grad_norm": 0.18610195815563202,
      "learning_rate": 3.454545454545455e-05,
      "loss": 2.1793,
      "step": 42
    },
    {
      "epoch": 0.034106682530239935,
      "grad_norm": 0.16388356685638428,
      "learning_rate": 3.272727272727273e-05,
      "loss": 2.0387,
      "step": 43
    },
    {
      "epoch": 0.03489986119373389,
      "grad_norm": 0.15558284521102905,
      "learning_rate": 3.090909090909091e-05,
      "loss": 2.0052,
      "step": 44
    },
    {
      "epoch": 0.03569303985722784,
      "grad_norm": 0.1736496239900589,
      "learning_rate": 2.909090909090909e-05,
      "loss": 2.2374,
      "step": 45
    },
    {
      "epoch": 0.03648621852072179,
      "grad_norm": 0.17933879792690277,
      "learning_rate": 2.7272727272727273e-05,
      "loss": 2.1439,
      "step": 46
    },
    {
      "epoch": 0.037279397184215746,
      "grad_norm": 0.17916561663150787,
      "learning_rate": 2.5454545454545454e-05,
      "loss": 2.282,
      "step": 47
    },
    {
      "epoch": 0.0380725758477097,
      "grad_norm": 0.1728973239660263,
      "learning_rate": 2.3636363636363637e-05,
      "loss": 2.2805,
      "step": 48
    },
    {
      "epoch": 0.03886575451120365,
      "grad_norm": 0.18513067066669464,
      "learning_rate": 2.1818181818181818e-05,
      "loss": 2.202,
      "step": 49
    },
    {
      "epoch": 0.039658933174697604,
      "grad_norm": 0.17480097711086273,
      "learning_rate": 2e-05,
      "loss": 2.2178,
      "step": 50
    },
    {
      "epoch": 0.04045211183819155,
      "grad_norm": 0.16613848507404327,
      "learning_rate": 1.8181818181818182e-05,
      "loss": 2.1566,
      "step": 51
    },
    {
      "epoch": 0.0412452905016855,
      "grad_norm": 0.178153857588768,
      "learning_rate": 1.6363636363636366e-05,
      "loss": 2.2078,
      "step": 52
    },
    {
      "epoch": 0.042038469165179455,
      "grad_norm": 0.1832481026649475,
      "learning_rate": 1.4545454545454545e-05,
      "loss": 2.0936,
      "step": 53
    },
    {
      "epoch": 0.04283164782867341,
      "grad_norm": 0.1671558916568756,
      "learning_rate": 1.2727272727272727e-05,
      "loss": 2.305,
      "step": 54
    },
    {
      "epoch": 0.04362482649216736,
      "grad_norm": 0.16498418152332306,
      "learning_rate": 1.0909090909090909e-05,
      "loss": 2.0962,
      "step": 55
    },
    {
      "epoch": 0.044418005155661314,
      "grad_norm": 0.1718718409538269,
      "learning_rate": 9.090909090909091e-06,
      "loss": 2.3143,
      "step": 56
    },
    {
      "epoch": 0.045211183819155266,
      "grad_norm": 0.17324374616146088,
      "learning_rate": 7.272727272727272e-06,
      "loss": 2.1803,
      "step": 57
    },
    {
      "epoch": 0.04600436248264922,
      "grad_norm": 0.17875775694847107,
      "learning_rate": 5.4545454545454545e-06,
      "loss": 2.1346,
      "step": 58
    },
    {
      "epoch": 0.04679754114614317,
      "grad_norm": 0.18000048398971558,
      "learning_rate": 3.636363636363636e-06,
      "loss": 2.2207,
      "step": 59
    },
    {
      "epoch": 0.04759071980963712,
      "grad_norm": 0.17751239240169525,
      "learning_rate": 1.818181818181818e-06,
      "loss": 2.2862,
      "step": 60
    }
  ],
  "logging_steps": 1,
  "max_steps": 60,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 6882985986097152.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
