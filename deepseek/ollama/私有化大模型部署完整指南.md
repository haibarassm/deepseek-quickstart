# ç§æœ‰åŒ–å¤§æ¨¡å‹éƒ¨ç½²å®Œæ•´æŒ‡å—

## ğŸ“š æ–‡æ¡£ç›®å½•
1. [WSLç¯å¢ƒä¸“å±é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ](#wslç¯å¢ƒä¸“å±é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ)
2. [Linuxé€šç”¨éƒ¨ç½²é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ](#linuxé€šç”¨éƒ¨ç½²é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ)
3. [Dockerç½‘ç»œä¸å®¹å™¨åŒ–é—®é¢˜](#dockerç½‘ç»œä¸å®¹å™¨åŒ–é—®é¢˜)
4. [Ollamaæ¨¡å‹ç®¡ç†ä¸éƒ¨ç½²](#ollamaæ¨¡å‹ç®¡ç†ä¸éƒ¨ç½²)
5. [Open WebUIå‰ç«¯éƒ¨ç½²é—®é¢˜](#open-webuiå‰ç«¯éƒ¨ç½²é—®é¢˜)
6. [ä¸åŒéƒ¨ç½²ç¯å¢ƒå¯¹æ¯”åˆ†æ](#ä¸åŒéƒ¨ç½²ç¯å¢ƒå¯¹æ¯”åˆ†æ)
7. [ç¡¬ä»¶ä¸æ“ä½œç³»ç»Ÿé€‰æ‹©æŒ‡å—](#ç¡¬ä»¶ä¸æ“ä½œç³»ç»Ÿé€‰æ‹©æŒ‡å—)
8. [æ€§èƒ½ä¼˜åŒ–ä¸è°ƒä¼˜å»ºè®®](#æ€§èƒ½ä¼˜åŒ–ä¸è°ƒä¼˜å»ºè®®)

---

## ğŸ–¥ï¸ WSLç¯å¢ƒä¸“å±é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

<div style="background-color: #e8f4f8; padding: 15px; border-radius: 5px; border-left: 4px solid #2196F3;">
<strong>ğŸ’™ WSLä¸“å±é—®é¢˜</strong> - ä»…åœ¨Windows Subsystem for Linuxç¯å¢ƒä¸­å‡ºç°
</div>

### 1. ç½‘ç»œè¿æ¥é—®é¢˜
#### é—®é¢˜ç°è±¡
- å®¹å™¨æ— æ³•è®¿é—®å¤–ç½‘ï¼Œç‰¹åˆ«æ˜¯HuggingFaceã€GitHubç­‰æµ·å¤–èµ„æº
- `Network is unreachable` æˆ– `SSLè¿æ¥å¤±è´¥` é”™è¯¯
- Dockerå®¹å™¨å†…DNSè§£æå¤±è´¥

#### æ ¹æœ¬åŸå› 
- **WSL2çš„NATç½‘ç»œæ¶æ„**ï¼šWSL2ä½¿ç”¨è™šæ‹Ÿç½‘ç»œï¼Œä¸Windowsä¸»æœºç½‘ç»œæ ˆä¸å®Œå…¨å…±äº«
- **VPNç©¿é€é—®é¢˜**ï¼šWindowsä¸»æœºå¼€å¯VPNæ—¶ï¼ŒWSL2ç½‘ç»œæ— æ³•è‡ªåŠ¨ç»§æ‰¿ä»£ç†è®¾ç½®
- **é˜²ç«å¢™è§„åˆ™é™åˆ¶**ï¼šWindows Defenderé˜²ç«å¢™å¯èƒ½é˜»æ­¢WSL2çš„ç½‘ç»œè®¿é—®

#### è§£å†³æ–¹æ¡ˆ
```bash
# è§£å†³æ–¹æ¡ˆ1ï¼šé…ç½®WSL2ä½¿ç”¨Windowsä»£ç†
# åœ¨WSL2ä¸­åˆ›å»ºæˆ–ç¼–è¾‘ ~/.bashrc æˆ– ~/.zshrc
export host_ip=$(cat /etc/resolv.conf | grep nameserver | awk '{print $2}')
export http_proxy="http://$host_ip:7890"
export https_proxy="http://$host_ip:7890"
export all_proxy="socks5://$host_ip:7890"

# è§£å†³æ–¹æ¡ˆ2ï¼šä¿®æ”¹WSL2ç½‘ç»œæ¨¡å¼
# åˆ›å»ºæˆ–ç¼–è¾‘ %USERPROFILE%\.wslconfig
[wsl2]
networkingMode=mirrored  # å¯ç”¨é•œåƒç½‘ç»œæ¨¡å¼
dnsTunneling=true
firewall=false
autoProxy=true

# è§£å†³æ–¹æ¡ˆ3ï¼šé‡å¯WSLç½‘ç»œ
wsl --shutdown
# ç­‰å¾…30ç§’åé‡æ–°å¯åŠ¨
```
### 2. ç«¯å£æ˜ å°„ä¸è®¿é—®é—®é¢˜
#### é—®é¢˜ç°è±¡
- `localhost:3000` æ— æ³•è®¿é—®å®¹å™¨æœåŠ¡
- ç«¯å£æ˜¾ç¤ºå·²ç»‘å®šä½†è¿æ¥è¢«æ‹’ç»
- åªèƒ½é€šè¿‡WSLå†…éƒ¨IPè®¿é—®

#### è§£å†³æ–¹æ¡ˆ
```bash
# è·å–WSL2çš„IPåœ°å€
ip addr show eth0 | grep inet | awk '{print $2}' | cut -d/ -f1

# ä½¿ç”¨Windowsæµè§ˆå™¨è®¿é—®WSL2 IP
# ä¾‹å¦‚ï¼šhttp://172.21.144.1:3000

# æˆ–è€…é…ç½®ç«¯å£è½¬å‘
# åœ¨PowerShellï¼ˆç®¡ç†å‘˜ï¼‰ä¸­æ‰§è¡Œ
netsh interface portproxy add v4tov4 listenport=3000 listenaddress=0.0.0.0 connectport=3000 connectaddress=$(wsl hostname -I).Trim()
```
### 3. GPUæ”¯æŒé—®é¢˜
#### é—®é¢˜ç°è±¡
- Dockerå®¹å™¨æ— æ³•è¯†åˆ«NVIDIA GPU
- `nvidia-smi` åœ¨å®¹å™¨ä¸­æ— æ³•æ‰§è¡Œ
- CUDAåŠ é€Ÿä¸å¯ç”¨

#### è§£å†³æ–¹æ¡ˆ
```powershell
# åœ¨Windows PowerShellä¸­æ£€æŸ¥GPUæ”¯æŒ
wsl --update
# ç¡®ä¿å®‰è£…WSL2å†…æ ¸æ›´æ–°

# å®‰è£…WSL2çš„CUDAé©±åŠ¨
# è®¿é—®NVIDIAå®˜ç½‘ä¸‹è½½é€‚ç”¨äºWSL2çš„CUDAé©±åŠ¨

# åœ¨WSL2ä¸­éªŒè¯
nvidia-smi

# Docker GPUæ”¯æŒ
docker run --rm --gpus all nvidia/cuda:12.3.1-base-ubuntu22.04 nvidia-smi
```
### 4. æ–‡ä»¶ç³»ç»Ÿæ€§èƒ½é—®é¢˜
#### é—®é¢˜ç°è±¡
- è®¿é—® `/mnt/c/` ä¸‹çš„æ–‡ä»¶é€Ÿåº¦ææ…¢
- å¤§æ¨¡å‹æ–‡ä»¶åŠ è½½æ—¶é—´é•¿
- I/Oæ€§èƒ½ç“¶é¢ˆæ˜æ˜¾

#### è§£å†³æ–¹æ¡ˆ
```bash
# å°†é¡¹ç›®å’Œæ•°æ®æ”¾åœ¨WSL2åŸç”Ÿæ–‡ä»¶ç³»ç»Ÿä¸­
# ä¸è¦ä½¿ç”¨ /mnt/c/ï¼Œè€Œæ˜¯ä½¿ç”¨ /home/username/ ç›®å½•

# åˆ›å»ºç¬¦å·é“¾æ¥ï¼ˆå¦‚æœå¿…é¡»ä½¿ç”¨Windowsæ–‡ä»¶ï¼‰
ln -s /mnt/c/Users/sherry/Projects ~/projects

# æˆ–è€…é…ç½®WSL2çš„å…ƒæ•°æ®é€‰é¡¹
# ç¼–è¾‘ %USERPROFILE%\.wslconfig
[wsl2]
metadata=true
```
### 5. å†…å­˜å’ŒCPUé™åˆ¶é—®é¢˜
#### é—®é¢˜ç°è±¡
- WSL2å ç”¨è¿‡å¤šWindowså†…å­˜
- å¤§æ¨¡å‹è¿è¡Œæ—¶å†…å­˜ä¸è¶³
- CPUä½¿ç”¨ç‡è¿‡é«˜å¯¼è‡´ç³»ç»Ÿå¡é¡¿

#### è§£å†³æ–¹æ¡ˆ
```ini
# ç¼–è¾‘ %USERPROFILE%\.wslconfig é™åˆ¶èµ„æºä½¿ç”¨
[wsl2]
memory=12GB        # é™åˆ¶å†…å­˜ä½¿ç”¨
processors=6       # é™åˆ¶CPUæ ¸å¿ƒæ•°
swap=4GB           # è®¾ç½®äº¤æ¢ç©ºé—´
localhostForwarding=true
```
## ğŸ§ Linuxé€šç”¨éƒ¨ç½²é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

<div style="background-color: #f0f9eb; padding: 15px; border-radius: 5px; border-left: 4px solid #4CAF50;">
<strong>ğŸ’š Linuxé€šç”¨é—®é¢˜</strong> - åœ¨æ‰€æœ‰Linuxå‘è¡Œç‰ˆä¸­å¯èƒ½å‡ºç°
</div>

### 1. ç³»ç»Ÿä¾èµ–é—®é¢˜
#### å¸¸è§ç¼ºå¤±ä¾èµ–
```bash
# Ubuntu/Debian ç³»ç»Ÿ
sudo apt update
sudo apt install -y \
    build-essential \
    curl \
    wget \
    git \
    python3-pip \
    python3-venv \
    libssl-dev \
    libffi-dev \
    libxml2-dev \
    libxslt1-dev \
    zlib1g-dev \
    libjpeg-dev \
    libpng-dev \
    libopenblas-dev \
    libomp-dev

# CentOS/RHEL ç³»ç»Ÿ
sudo yum groupinstall "Development Tools"
sudo yum install -y \
    openssl-devel \
    bzip2-devel \
    libffi-devel \
    sqlite-devel \
    python3-devel \
    wget \
    git
```
### 2. Pythonç¯å¢ƒé—®é¢˜
#### è™šæ‹Ÿç¯å¢ƒé…ç½®
```bash
# åˆ›å»ºç‹¬ç«‹çš„Pythonè™šæ‹Ÿç¯å¢ƒ
python3 -m venv ~/ai-env
source ~/ai-env/bin/activate

# å®‰è£…åŸºç¡€ä¾èµ–
pip install --upgrade pip setuptools wheel

# å®‰è£…PyTorchï¼ˆæ ¹æ®CUDAç‰ˆæœ¬é€‰æ‹©ï¼‰
# CUDA 12.x
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# CPU only
pip install torch torchvision torchaudio
```
### 3. æ¨¡å‹ä¸‹è½½ä¸å­˜å‚¨é—®é¢˜
#### HuggingFaceè®¿é—®é—®é¢˜
```bash
# ä½¿ç”¨å›½å†…é•œåƒæº
export HF_ENDPOINT="https://hf-mirror.com"

# é…ç½®HuggingFaceç¼“å­˜ç›®å½•
export HF_HOME="/path/to/your/huggingface/cache"
export TRANSFORMERS_CACHE="$HF_HOME"
export HF_DATASETS_CACHE="$HF_HOME"

# é¢„ä¸‹è½½å¸¸ç”¨æ¨¡å‹
python -c "from transformers import AutoModel; AutoModel.from_pretrained('bert-base-uncased')"
```
#### å¤§æ¨¡å‹åˆ†å—ä¸‹è½½
```bash
# ä½¿ç”¨huggingface-cliåˆ†æ®µä¸‹è½½
pip install huggingface-hub[hf_transfer]

# å¯ç”¨æ–­ç‚¹ç»­ä¼ 
export HF_HUB_ENABLE_HF_TRANSFER=1

# ä¸‹è½½å¤§æ¨¡å‹
huggingface-cli download meta-llama/Llama-2-7b-chat-hf --local-dir ./llama2-7b
```
### 4. æƒé™ä¸å®‰å…¨é…ç½®
#### Dockerérootç”¨æˆ·è¿è¡Œ
```bash
# åˆ›å»ºdockerç”¨æˆ·ç»„
sudo groupadd docker
sudo usermod -aG docker $USER

# é‡å¯dockeræœåŠ¡
sudo systemctl restart docker

# éªŒè¯érootæƒé™
docker run hello-world

# è®¾ç½®Dockeræ•°æ®ç›®å½•æƒé™
sudo chmod 777 /var/run/docker.sock
```
#### æ–‡ä»¶æƒé™ç®¡ç†
```bash
# åˆ›å»ºä¸“ç”¨çš„æ•°æ®ç›®å½•
sudo mkdir -p /data/models /data/datasets /data/logs
sudo chown -R $USER:$USER /data

# è®¾ç½®åˆé€‚çš„æƒé™
find /data -type d -exec chmod 755 {} \;
find /data -type f -exec chmod 644 {} \;
```
### 5. ç³»ç»Ÿä¼˜åŒ–é…ç½®
#### å†…æ ¸å‚æ•°è°ƒæ•´
```bash
# ç¼–è¾‘ /etc/sysctl.conf
sudo nano /etc/sysctl.conf

# æ·»åŠ ä»¥ä¸‹é…ç½®
vm.swappiness = 10
vm.dirty_ratio = 60
vm.dirty_background_ratio = 2
net.core.rmem_max = 134217728
net.core.wmem_max = 134217728
net.ipv4.tcp_rmem = 4096 87380 134217728
net.ipv4.tcp_wmem = 4096 65536 134217728

# åº”ç”¨é…ç½®
sudo sysctl -p
```
#### æ–‡ä»¶ç³»ç»Ÿä¼˜åŒ–
```bash
# æ£€æŸ¥å¹¶æŒ‚è½½é€‰é¡¹ä¼˜åŒ–
# ç¼–è¾‘ /etc/fstab
# æ·»åŠ noatime,nodiratimeé€‰é¡¹
/dev/sdb1 /data ext4 defaults,noatime,nodiratime 0 0
```
## ğŸ³ Dockerç½‘ç»œä¸å®¹å™¨åŒ–é—®é¢˜

<div style="background-color: #fff3e0; padding: 15px; border-radius: 5px; border-left: 4px solid #FF9800;">
<strong>ğŸ§¡ Dockeré€šç”¨é—®é¢˜</strong> - åœ¨Dockerå®¹å™¨åŒ–éƒ¨ç½²ä¸­å‡ºç°
</div>

### 1. Dockerç½‘ç»œæ¨¡å¼è¯¦è§£
#### å››ç§ç½‘ç»œæ¨¡å¼å¯¹æ¯”

| ç½‘ç»œæ¨¡å¼ | å‘½ä»¤å‚æ•° | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|---------|---------|------|----------|
| **Bridge** | `-p å®¿ä¸»æœºç«¯å£:å®¹å™¨ç«¯å£` | é»˜è®¤æ¨¡å¼ï¼Œå®¹å™¨é—´éš”ç¦» | ç”Ÿäº§ç¯å¢ƒï¼Œå¤šå®¹å™¨åº”ç”¨ |
| **Host** | `--network host` | å…±äº«å®¿ä¸»æœºç½‘ç»œæ ˆ | é«˜æ€§èƒ½éœ€æ±‚ï¼Œå•å®¹å™¨åº”ç”¨ |
| **None** | `--network none` | æ— ç½‘ç»œè¿æ¥ | å®‰å…¨éš”ç¦»ï¼Œæµ‹è¯•ç¯å¢ƒ |
| **è‡ªå®šä¹‰** | `--network è‡ªå®šä¹‰ç½‘ç»œ` | ç”¨æˆ·å®šä¹‰ç½‘ç»œ | å¾®æœåŠ¡æ¶æ„ï¼Œå¤æ‚ç½‘ç»œ |

#### ç½‘ç»œé—®é¢˜è¯Šæ–­å‘½ä»¤
```bash
# æŸ¥çœ‹Dockerç½‘ç»œ
docker network ls
docker network inspect bridge

# æŸ¥çœ‹å®¹å™¨ç½‘ç»œé…ç½®
docker inspect <container_id> | grep -A 10 "NetworkSettings"

# æµ‹è¯•å®¹å™¨ç½‘ç»œè¿é€šæ€§
docker exec <container> ping -c 3 8.8.8.8
docker exec <container> curl -v https://huggingface.co

# æŸ¥çœ‹ç«¯å£ç»‘å®š
docker port <container>
netstat -tlnp | grep docker
```
### 2. å¸¸è§çš„Dockerç½‘ç»œé—®é¢˜
#### é—®é¢˜1ï¼šç«¯å£å†²çªæˆ–æœªç»‘å®š
```bash
# é”™è¯¯ç°è±¡ï¼šPORTSåˆ—ä¸ºç©º
CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES

# è§£å†³æ–¹æ¡ˆï¼šç¡®ä¿ä½¿ç”¨-på‚æ•°
docker run -d -p 3000:8080 --name webui open-webui

# æˆ–è€…æ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨
sudo lsof -i :3000
sudo lsof -i :8080
```
#### é—®é¢˜2ï¼šå®¹å™¨é—´ç½‘ç»œä¸é€š
```bash
# åˆ›å»ºè‡ªå®šä¹‰ç½‘ç»œ
docker network create ai-network

# å°†å®¹å™¨è¿æ¥åˆ°åŒä¸€ç½‘ç»œ
docker run -d --network ai-network --name ollama ollama/ollama
docker run -d --network ai-network -p 3000:8080 --name webui open-webui

# å®¹å™¨é—´é€šè¿‡æœåŠ¡åé€šä¿¡
# åœ¨webuiå®¹å™¨ä¸­å¯ä»¥é€šè¿‡ http://ollama:11434 è®¿é—®Ollama http://ollama:11434 è®¿é—®Ollama
```
#### é—®é¢˜3ï¼šDNSè§£æå¤±è´¥
```bash
# é…ç½®è‡ªå®šä¹‰DNS
docker run -d \
  --dns 8.8.8.8 \
  --dns 114.114.114.114 \
  --add-host host.docker.internal:host-gateway \
  -p 3000:8080 \
  open-webui

# æˆ–è€…åœ¨docker daemoné…ç½®ä¸­è®¾ç½®
# ç¼–è¾‘ /etc/docker/daemon.json
{
  "dns": ["8.8.8.8", "114.114.114.114"],
  "dns-opts": ["timeout:2", "attempts:3"]
}
```
### 3. Docker Composeç½‘ç»œé…ç½®
```yaml
version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    networks:
      - ai-net
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped
    # GPUæ”¯æŒ
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    depends_on:
      - ollama
    networks:
      - ai-net
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY}
      - HF_ENDPOINT=https://hf-mirror.com
    volumes:
      - open-webui:/app/backend/data
    restart: unless-stopped

networks:
  ai-net:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

volumes:
  ollama_data:
  open-webui:
```
### 4. Dockerå­˜å‚¨ä¸å·ç®¡ç†
#### æŒä¹…åŒ–æ•°æ®é…ç½®
```bash
# åˆ›å»ºå‘½åå·
docker volume create ollama_data
docker volume create webui_data

# ç»‘å®šå®¿ä¸»æœºç›®å½•
docker run -d \
  -v /path/on/host:/path/in/container \
  -v model_cache:/app/models \
  ollama/ollama

# æŸ¥çœ‹å·ä½¿ç”¨æƒ…å†µ
docker volume ls
docker volume inspect <volume_name>
```
#### æ•°æ®å¤‡ä»½ä¸æ¢å¤
```bash
# å¤‡ä»½Dockerå·
docker run --rm -v ollama_data:/data -v $(pwd):/backup alpine tar czf /backup/ollama_backup.tar.gz -C /data .

# æ¢å¤Dockerå·
docker run --rm -v ollama_data:/data -v $(pwd):/backup alpine sh -c "rm -rf /data/* && tar xzf /backup/ollama_backup.tar.gz -C /data"

# è¿ç§»æ•°æ®åˆ°æ–°æœåŠ¡å™¨
# 1. å¤‡ä»½
docker exec ollama ollama list
tar czf ollama_models.tar.gz /var/lib/docker/volumes/ollama_data/_data

# 2. ä¼ è¾“åˆ°æ–°æœåŠ¡å™¨
scp ollama_models.tar.gz user@new-server:/tmp/

# 3. æ¢å¤
docker volume create ollama_data
docker run --rm -v ollama_data:/data -v /tmp:/backup alpine tar xzf /backup/ollama_models.tar.gz -C /data
```
### 5. Dockerèµ„æºé™åˆ¶ä¸ä¼˜åŒ–
#### CPUå’Œå†…å­˜é™åˆ¶
```bash
# é™åˆ¶å®¹å™¨èµ„æºä½¿ç”¨
docker run -d \
  --cpus="2.0" \           # é™åˆ¶ä½¿ç”¨2ä¸ªCPUæ ¸å¿ƒ
  --memory="8g" \          # é™åˆ¶å†…å­˜8GB
  --memory-swap="12g" \    # æ€»å†…å­˜+swap 12GB
  --pids-limit=100 \       # é™åˆ¶è¿›ç¨‹æ•°
  -p 3000:8080 \
  open-webui

# ç›‘æ§å®¹å™¨èµ„æºä½¿ç”¨
docker stats
docker stats --format "table {{.Name}}\t{{.CPUPerc}}\t{{.MemUsage}}"
```
#### GPUèµ„æºç®¡ç†
```bash
# æŒ‡å®šä½¿ç”¨ç‰¹å®šGPU
docker run -d \
  --gpus '"device=0,1"' \    # ä½¿ç”¨GPU 0å’Œ1
  --name gpu-container \
  nvidia/cuda:12.3.1-base

# æˆ–è€…æŒ‡å®šGPUè®¡ç®—èƒ½åŠ›
docker run -d \
  --gpus 'all,capabilities=compute,utility' \
  --name ollama-gpu \
  ollama/ollama:latest

# æ£€æŸ¥GPUä½¿ç”¨æƒ…å†µ
docker exec gpu-container nvidia-smi
```
## ğŸ¤– Ollamaæ¨¡å‹ç®¡ç†ä¸éƒ¨ç½²

<div style="background-color: #fce4ec; padding: 15px; border-radius: 5px; border-left: 4px solid #E91E63;">
<strong>ğŸ’– Ollamaç›¸å…³é—®é¢˜</strong> - å¤§æ¨¡å‹æœåŠ¡ç®¡ç†
</div>

### 1. Ollamaå®‰è£…ä¸é…ç½®
#### å¤šç§å®‰è£…æ–¹å¼
```bash
# æ–¹å¼1ï¼šä½¿ç”¨å®˜æ–¹è„šæœ¬ï¼ˆLinux/macOSï¼‰
curl -fsSL https://ollama.com/install.sh | sh

# æ–¹å¼2ï¼šæ‰‹åŠ¨å®‰è£…
# ä¸‹è½½æœ€æ–°ç‰ˆæœ¬
wget https://ollama.com/download/ollama-linux-amd64
chmod +x ollama-linux-amd64
sudo mv ollama-linux-amd64 /usr/local/bin/ollama

# æ–¹å¼3ï¼šDockerå®‰è£…
docker run -d \
  -v ollama:/root/.ollama \
  -p 11434:11434 \
  --name ollama \
  --restart always \
  ollama/ollama:latest

# æ–¹å¼4ï¼šWindowså®‰è£…
# ä¸‹è½½ https://ollama.com/download/OllamaSetup.exe
# æˆ–ä½¿ç”¨WSL2ï¼šåœ¨WSL2ä¸­è¿è¡ŒLinuxå®‰è£…è„šæœ¬
```
#### ç³»ç»ŸæœåŠ¡é…ç½®
```bash
# é…ç½®Ollamaä¸ºç³»ç»ŸæœåŠ¡ï¼ˆLinuxï¼‰
sudo systemctl enable ollama
sudo systemctl start ollama
sudo systemctl status ollama

# æŸ¥çœ‹æœåŠ¡æ—¥å¿—
journalctl -u ollama -f
sudo tail -f /var/log/ollama/ollama.log

# é…ç½®ç¯å¢ƒå˜é‡
export OLLAMA_HOST=0.0.0.0  # å…è®¸è¿œç¨‹è®¿é—®
export OLLAMA_MODELS=/data/models  # è‡ªå®šä¹‰æ¨¡å‹ç›®å½•
export OLLAMA_NUM_PARALLEL=2  # å¹¶è¡Œè¯·æ±‚æ•°
```
### 2. æ¨¡å‹ä¸‹è½½ä¸ç®¡ç†
#### åŸºç¡€æ¨¡å‹æ“ä½œ
```bash
# æŸ¥çœ‹å¯ç”¨æ¨¡å‹
ollama list

# ä¸‹è½½æ¨¡å‹ï¼ˆè‡ªåŠ¨é€‰æ‹©æœ€ä½³ç‰ˆæœ¬ï¼‰
ollama pull llama2
ollama pull mistral
ollama pull qwen2.5:7b

# ä¸‹è½½æŒ‡å®šç‰ˆæœ¬
ollama pull llama2:13b
ollama pull mistral:7b-instruct-q4_K_M

# è¿è¡Œæ¨¡å‹
ollama run llama2
ollama run mistral "ä½ å¥½ï¼Œä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"

# åˆ é™¤æ¨¡å‹
ollama rm llama2
```
#### æ¨¡å‹ä¼˜åŒ–å‚æ•°
```bash
# æŸ¥çœ‹æ¨¡å‹è¯¦æƒ…
ollama show llama2 --modelfile

# åˆ›å»ºè‡ªå®šä¹‰æ¨¡å‹é…ç½®
ollama create my-llama -f ./Modelfile

# Modelfileç¤ºä¾‹
FROM llama2:13b

# è®¾ç½®ç³»ç»Ÿæç¤ºè¯
SYSTEM """
ä½ æ˜¯ä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œè¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
å›ç­”è¦è¯¦ç»†ã€å‡†ç¡®ã€æœ‰å¸®åŠ©ã€‚
"""

# è®¾ç½®å‚æ•°
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER num_ctx 4096
PARAMETER num_predict 512
```

### 3. æ€§èƒ½ä¼˜åŒ–é…ç½®
#### é‡åŒ–æ¨¡å‹é€‰æ‹©

<B>ä¸åŒé‡åŒ–çº§åˆ«å¯¹æ¯”</B>
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ é‡åŒ–çº§åˆ«        â”‚ æ¨¡å‹å¤§å°   â”‚ å†…å­˜éœ€æ±‚   â”‚ æ¨ç†é€Ÿåº¦     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Q2_K           â”‚ æœ€å°       â”‚ æœ€ä½       â”‚ æœ€å¿«         â”‚
â”‚ Q3_K_S         â”‚ è¾ƒå°       â”‚ è¾ƒä½       â”‚ è¾ƒå¿«         â”‚
â”‚ Q4_0           â”‚ å°         â”‚ ä½         â”‚ å¿«           â”‚
â”‚ Q4_K_M         â”‚ ä¸­ç­‰       â”‚ ä¸­         â”‚ ä¸­ç­‰         â”‚
â”‚ Q5_0           â”‚ è¾ƒå¤§       â”‚ è¾ƒé«˜       â”‚ è¾ƒæ…¢         â”‚
â”‚ Q6_K           â”‚ å¤§         â”‚ é«˜         â”‚ æ…¢           â”‚
â”‚ Q8_0           â”‚ æœ€å¤§       â”‚ æœ€é«˜       â”‚ æœ€æ…¢         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
<B> æ¨èé…ç½®</B>
- 8GBå†…å­˜ï¼šQ4_0 æˆ– Q4_K_M
- 16GBå†…å­˜ï¼šQ5_0 æˆ– Q5_K_M
- 32GB+å†…å­˜ï¼šQ6_K æˆ– Q8_0

#### å¯åŠ¨å‚æ•°ä¼˜åŒ–
```bash
# ä¼˜åŒ–å¯åŠ¨è„šæœ¬
# ç¼–è¾‘ /etc/systemd/system/ollama.service
[Service]
Environment="OLLAMA_NUM_PARALLEL=4"
Environment="OLLAMA_MAX_LOADED_MODELS=3"
Environment="OLLAMA_KEEP_ALIVE=5m"
ExecStart=/usr/local/bin/ollama serve

# æˆ–è€…ä½¿ç”¨dockerç¯å¢ƒå˜é‡
docker run -d \
  -e OLLAMA_NUM_PARALLEL=4 \
  -e OLLAMA_MAX_LOADED_MODELS=3 \
  -e OLLAMA_KEEP_ALIVE=300 \
  -p 11434:11434 \
  ollama/ollama
```
### 4. APIé›†æˆä¸ä½¿ç”¨
#### REST APIç¤ºä¾‹
```python
import requests
import json

# åŸºç¡€APIè°ƒç”¨
def ollama_generate(model="llama2", prompt="Hello", stream=False):
    url = "http://localhost:11434/api/generate"
    data = {
        "model": model,
        "prompt": prompt,
        "stream": stream,
        "options": {
            "temperature": 0.7,
            "top_p": 0.9,
            "num_predict": 512
        }
    }
    
    response = requests.post(url, json=data, stream=stream)
    
    if stream:
        for line in response.iter_lines():
            if line:
                yield json.loads(line.decode('utf-8'))
    else:
        return response.json()

# ä½¿ç”¨ç¤ºä¾‹
response = ollama_generate("llama2", "è§£é‡Šä¸€ä¸‹é‡å­è®¡ç®—")
print(response["response"])
```
#### æµå¼å“åº”å¤„ç†
```python
# æµå¼å“åº”
def stream_response(prompt):
    for chunk in ollama_generate(prompt=prompt, stream=True):
        if "response" in chunk:
            print(chunk["response"], end="", flush=True)
        if chunk.get("done", False):
            print()  # æ¢è¡Œ

# èŠå¤©æ¨¡å¼
def chat_conversation():
    messages = []
    
    while True:
        user_input = input("ä½ : ")
        if user_input.lower() in ['exit', 'quit', 'bye']:
            break
            
        messages.append({"role": "user", "content": user_input})
        
        print("AI: ", end="")
        response = ""
        for chunk in ollama_generate(prompt=user_input, stream=True):
            if "response" in chunk:
                response += chunk["response"]
                print(chunk["response"], end="", flush=True)
        
        messages.append({"role": "assistant", "content": response})
        print()
```
### 5. æ¨¡å‹ç®¡ç†ä¸ç›‘æ§
#### è‡ªåŠ¨åŒ–ç®¡ç†è„šæœ¬
```bash
#!/bin/bash
# ollama_manager.sh

MODEL_DIR="/data/ollama/models"
BACKUP_DIR="/data/backup/ollama"
LOG_FILE="/var/log/ollama/manager.log"

# å¤‡ä»½æ‰€æœ‰æ¨¡å‹
backup_models() {
    echo "$(date): å¼€å§‹å¤‡ä»½æ¨¡å‹" >> "$LOG_FILE"
    
    # åœæ­¢ollamaæœåŠ¡
    systemctl stop ollama
    
    # å¤‡ä»½æ¨¡å‹æ–‡ä»¶
    tar czf "$BACKUP_DIR/models_$(date +%Y%m%d).tar.gz" -C "$MODEL_DIR" .
    
    # é‡æ–°å¯åŠ¨æœåŠ¡
    systemctl start ollama
    
    echo "$(date): æ¨¡å‹å¤‡ä»½å®Œæˆ" >> "$LOG_FILE"
}

# æ¸…ç†æ—§æ¨¡å‹
cleanup_old_models() {
    # ä¿ç•™æœ€è¿‘7å¤©çš„æ¨¡å‹å¤‡ä»½
    find "$BACKUP_DIR" -name "models_*.tar.gz" -mtime +7 -delete
    
    # æ¸…ç†æœªä½¿ç”¨çš„æ¨¡å‹
    ollama list | awk 'NR>1 {print $1}' | while read model; do
        last_used=$(stat -c %Y "$MODEL_DIR/$model" 2>/dev/null || echo 0)
        current_time=$(date +%s)
        days_unused=$(( (current_time - last_used) / 86400 ))
        
        if [ $days_unused -gt 30 ]; then
            echo "$(date): åˆ é™¤30å¤©æœªä½¿ç”¨çš„æ¨¡å‹: $model" >> "$LOG_FILE"
            ollama rm "$model"
        fi
    done
}

# ç›‘æ§æ¨¡å‹ä½¿ç”¨æƒ…å†µ
monitor_usage() {
    while true; do
        # æ£€æŸ¥å†…å­˜ä½¿ç”¨
        memory_usage=$(ollama ps | awk 'NR>1 {sum+=$4} END {print sum}')
        
        if [ "$memory_usage" -gt 8000 ]; then
            echo "$(date): è­¦å‘Šï¼šæ¨¡å‹å†…å­˜ä½¿ç”¨è¶…è¿‡8GB: ${memory_usage}MB" >> "$LOG_FILE"
        fi
        
        sleep 300  # 5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
    done
}
```#### å¥åº·æ£€æŸ¥ä¸ç›‘æ§
```bash
# å¥åº·æ£€æŸ¥è„šæœ¬
#!/bin/bash

# æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€
check_ollama_health() {
    # æ£€æŸ¥ç«¯å£
    if ! nc -z localhost 11434; then
        echo "ERROR: Ollamaç«¯å£æœªç›‘å¬"
        return 1
    fi
    
    # æ£€æŸ¥APIå“åº”
    response=$(curl -s -o /dev/null -w "%{http_code}" http://localhost:11434/api/tags)
    if [ "$response" != "200" ]; then
        echo "ERROR: Ollama APIå“åº”å¼‚å¸¸: $response"
        return 1
    fi
    
    # æ£€æŸ¥æ¨¡å‹åŠ è½½
    models=$(curl -s http://localhost:11434/api/tags | jq '.models | length')
    if [ "$models" -eq 0 ]; then
        echo "WARNING: æœªåŠ è½½ä»»ä½•æ¨¡å‹"
    fi
    
    echo "OK: OllamaæœåŠ¡è¿è¡Œæ­£å¸¸ï¼ŒåŠ è½½äº†${models}ä¸ªæ¨¡å‹"
    return 0
}

# Prometheusç›‘æ§æŒ‡æ ‡
generate_metrics() {
    cat << EOF
# HELP ollama_model_count å·²åŠ è½½æ¨¡å‹æ•°é‡
# TYPE ollama_model_count gauge
ollama_model_count $(curl -s http://localhost:11434/api/tags | jq '.models | length')

# HELP ollama_request_total æ€»è¯·æ±‚æ•°
# TYPE ollama_request_total counter
ollama_request_total $(grep -c "generate" /var/log/ollama/access.log 2>/dev/null || echo 0)
EOF
}
```
## ğŸŒ Open WebUIå‰ç«¯éƒ¨ç½²é—®é¢˜

<div style="background-color: #e8eaf6; padding: 15px; border-radius: 5px; border-left: 4px solid #3F51B5;">
<strong>ğŸ’œ Open WebUIç›¸å…³é—®é¢˜</strong> - Webç•Œé¢éƒ¨ç½²ä¸ç®¡ç†
</div>

### 1. Open WebUIå®‰è£…æ–¹å¼å¯¹æ¯”

| å®‰è£…æ–¹å¼ | å‘½ä»¤/æ–¹æ³• | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|---------|----------|------|------|----------|
| **Docker** | `docker run -p 3000:8080 open-webui` | ä¸€é”®éƒ¨ç½²ï¼Œéš”ç¦»æ€§å¥½ | éœ€è¦Dockerç¯å¢ƒ | ç”Ÿäº§ç¯å¢ƒ |
| **Docker Compose** | `docker-compose up -d` | å¤šæœåŠ¡ç¼–æ’ï¼Œé…ç½®ç»Ÿä¸€ | é…ç½®å¤æ‚ | å¤šç»„ä»¶éƒ¨ç½² |
| **æºç å®‰è£…** | `pip install -r requirements.txt` | çµæ´»å®šåˆ¶ï¼Œè°ƒè¯•æ–¹ä¾¿ | ä¾èµ–ç®¡ç†å¤æ‚ | å¼€å‘ç¯å¢ƒ |
| **Kubernetes** | `kubectl apply -f deploy.yaml` | é«˜å¯ç”¨ï¼Œå¼¹æ€§ä¼¸ç¼© | è¿ç»´å¤æ‚ | å¤§è§„æ¨¡éƒ¨ç½² |

### 2. Dockeréƒ¨ç½²è¯¦ç»†é…ç½®
```bash
# åŸºç¡€ç‰ˆæœ¬
docker run -d \
  -p 3000:8080 \
  -v open-webui:/app/backend/data \
  --name open-webui \
  --restart always \
  ghcr.io/open-webui/open-webui:main

# GPUæ”¯æŒç‰ˆæœ¬
docker run -d \
  --gpus all \
  -p 3000:8080 \
  --add-host=host.docker.internal:host-gateway \
  -v open-webui:/app/backend/data \
  --name open-webui \
  --restart always \
  ghcr.io/open-webui/open-webui:cuda

# å›½å†…ä¼˜åŒ–ç‰ˆæœ¬
docker run -d \
  -p 3000:8080 \
  -e HF_ENDPOINT=https://hf-mirror.com \
  -e ENABLE_RAG=false \
  -e WEBUI_SECRET_KEY=$(openssl rand -hex 32) \
  -v open-webui:/app/backend/data \
  --name open-webui \
  --restart unless-stopped \
  ghcr.io/open-webui/open-webui:main
  ```
### 3. ç¯å¢ƒå˜é‡é…ç½®è¯¦è§£
```bash
# å¿…éœ€çš„ç¯å¢ƒå˜é‡
export WEBUI_SECRET_KEY="your-secret-key-here"
export OLLAMA_BASE_URL="http://localhost:11434"

# å¯é€‰é…ç½®
export ENABLE_RAG="true"  # å¯ç”¨æ£€ç´¢å¢å¼º
export RAG_EMBEDDING_MODEL="all-MiniLM-L6-v2"
export MAX_FILE_SIZE="100"  # MB
export ALLOWED_FILE_TYPES="pdf,txt,docx,md"
export CORS_ALLOW_ORIGINS="http://localhost:3000,http://127.0.0.1:3000"

# æ•°æ®åº“é…ç½®
export DATABASE_URL="sqlite:///data/database.db"
# æˆ–ä½¿ç”¨PostgreSQL
export DATABASE_URL="postgresql://user:password@localhost:5432/openwebui"

# ç¼“å­˜é…ç½®
export REDIS_URL="redis://localhost:6379/0"
```
### 4. å¸¸è§éƒ¨ç½²é—®é¢˜è§£å†³

#### é—®é¢˜1ï¼šRAGåŠŸèƒ½æ— æ³•åŠ è½½æ¨¡å‹
```bash
# ç—‡çŠ¶ï¼šCannot determine model snapshot path
# åŸå› ï¼šæ— æ³•ä»HuggingFaceä¸‹è½½åµŒå…¥æ¨¡å‹

# è§£å†³æ–¹æ¡ˆ1ï¼šç¦ç”¨RAG
docker run -d \
  -e ENABLE_RAG=false \
  -p 3000:8080 \
  open-webui

# è§£å†³æ–¹æ¡ˆ2ï¼šä½¿ç”¨æœ¬åœ°æ¨¡å‹
# å…ˆä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°
mkdir -p ~/models
cd ~/models
git clone https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2

# æŒ‚è½½æ¨¡å‹ç›®å½•
docker run -d \
  -v ~/models:/models \
  -e RAG_EMBEDDING_MODEL_PATH=/models/all-MiniLM-L6-v2 \
  -p 3000:8080 \
  open-webui
  ```
#### é—®é¢˜2ï¼šå®¹å™¨å¥åº·æ£€æŸ¥å¤±è´¥
```bash
# ç—‡çŠ¶ï¼šå®¹å™¨çŠ¶æ€æ˜¾ç¤ºunhealthy

# è§£å†³æ–¹æ¡ˆï¼šè‡ªå®šä¹‰å¥åº·æ£€æŸ¥
docker run -d \
  -p 3000:8080 \
  --health-cmd="curl -f http://localhost:8080/api/v1/health || exit 1" \
  --health-interval=30s \
  --health-timeout=10s \
  --health-retries=3 \
  --name open-webui \
  open-webui

# æˆ–è€…ç¦ç”¨å¥åº·æ£€æŸ¥
docker run -d \
  --no-healthcheck \
  -p 3000:8080 \
  open-webui
```
#### é—®é¢˜3ï¼šä¸Šä¼ æ–‡ä»¶å¤§å°é™åˆ¶
```bash
# ä¿®æ”¹ä¸Šä¼ é™åˆ¶
docker run -d \
  -e MAX_FILE_SIZE="200" \  # 200MB
  -e MAX_UPLOAD_FILES="10" \
  -p 3000:8080 \
  open-webui

# å¦‚æœéœ€è¦æ›´å¤§çš„é™åˆ¶ï¼Œä¿®æ”¹nginxé…ç½®
# åˆ›å»ºè‡ªå®šä¹‰nginxé…ç½®
mkdir -p ~/openwebui-config
cat > ~/openwebui-config/nginx.conf << EOF
client_max_body_size 500M;
proxy_read_timeout 300s;
proxy_connect_timeout 75s;
EOF

# æŒ‚è½½é…ç½®
docker run -d \
  -v ~/openwebui-config/nginx.conf:/etc/nginx/conf.d/custom.conf:ro \
  -p 3000:8080 \
  open-webui
  ```

### 5. Nginxåå‘ä»£ç†é…ç½®
```nginx
# /etc/nginx/sites-available/open-webui
upstream open-webui {
    server 127.0.0.1:8080;
    keepalive 32;
}

server {
    listen 80;
    server_name ai.yourdomain.com;
    
    # é‡å®šå‘åˆ°HTTPS
    return 301 https://$server_name$request_uri;
}

server {
    listen 443 ssl http2;
    server_name ai.yourdomain.com;
    
    # SSLè¯ä¹¦
    ssl_certificate /etc/letsencrypt/live/ai.yourdomain.com/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/ai.yourdomain.com/privkey.pem;
    
    # SSLä¼˜åŒ–
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512;
    ssl_prefer_server_ciphers off;
    
    # å®‰å…¨å¤´éƒ¨
    add_header X-Frame-Options DENY;
    add_header X-Content-Type-Options nosniff;
    add_header X-XSS-Protection "1; mode=block";
    
    # ä¸Šä¼ é™åˆ¶
    client_max_body_size 100M;
    
    # WebSocketæ”¯æŒ
    location / {
        proxy_pass http://open-webui;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # è¶…æ—¶è®¾ç½®
        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;
    }
    
    # APIç«¯ç‚¹
    location /api/ {
        proxy_pass http://open-webui;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        
        # å¢åŠ APIè¶…æ—¶æ—¶é—´
        proxy_read_timeout 300s;
    }
}
```
### 6. å¤‡ä»½ä¸æ¢å¤
```bash
#!/bin/bash
# openwebui_backup.sh

BACKUP_DIR="/data/backup/openwebui"
DATE=$(date +%Y%m%d_%H%M%S)

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p "$BACKUP_DIR"

# å¤‡ä»½æ•°æ®åº“
docker exec open-webui sqlite3 /app/backend/data/database.db ".backup '/tmp/openwebui.db'"
docker cp open-webui:/tmp/openwebui.db "$BACKUP_DIR/openwebui_${DATE}.db"

# å¤‡ä»½ä¸Šä¼ çš„æ–‡ä»¶
tar czf "$BACKUP_DIR/uploads_${DATE}.tar.gz" -C /var/lib/docker/volumes/open-webui/_data/uploads .

# å¤‡ä»½é…ç½®
docker exec open-webui cat /app/backend/.env > "$BACKUP_DIR/env_${DATE}.txt"

# æ¸…ç†æ—§å¤‡ä»½ï¼ˆä¿ç•™æœ€è¿‘30å¤©ï¼‰
find "$BACKUP_DIR" -name "*.tar.gz" -mtime +30 -delete
find "$BACKUP_DIR" -name "*.db" -mtime +30 -delete
```
## ğŸ¢ ä¸åŒéƒ¨ç½²ç¯å¢ƒå¯¹æ¯”åˆ†æ

<div style="background-color: #f3e5f5; padding: 15px; border-radius: 5px; border-left: 4px solid #9C27B0;">
<strong>ğŸ’œ éƒ¨ç½²ç¯å¢ƒå¯¹æ¯”</strong> - æœ¬æœºã€äº‘æœåŠ¡å™¨ã€æ··åˆéƒ¨ç½²
</div>

### 1. éƒ¨ç½²ç¯å¢ƒå¯¹æ¯”è¡¨

| å¯¹æ¯”ç»´åº¦ | æœ¬æœºéƒ¨ç½² | äº‘æœåŠ¡å™¨éƒ¨ç½² | æ··åˆéƒ¨ç½² |
|---------|----------|-------------|----------|
| **æˆæœ¬** | ç¡¬ä»¶ä¸€æ¬¡æ€§æŠ•å…¥ | æŒ‰ä½¿ç”¨ä»˜è´¹ï¼Œçµæ´» | ä¸­ç­‰ï¼Œå¹³è¡¡æˆæœ¬ |
| **æ€§èƒ½** | å–å†³äºæœ¬åœ°ç¡¬ä»¶ | å¯å¼¹æ€§ä¼¸ç¼© | å¯éƒ¨åˆ†ä¼˜åŒ– |
| **ç½‘ç»œ** | å†…ç½‘é€Ÿåº¦å¿« | å…¬ç½‘è®¿é—®ï¼Œå¸¦å®½å¯é€‰ | å†…ç½‘+å…¬ç½‘ç»“åˆ |
| **å®‰å…¨** | ç‰©ç†æ§åˆ¶ï¼Œè¾ƒå®‰å…¨ | ä¾èµ–äº‘æœåŠ¡å•†å®‰å…¨ | ä¸­ç­‰ |
| **ç»´æŠ¤** | è‡ªå·±è´Ÿè´£ | éƒ¨åˆ†äº‘æœåŠ¡å•†æ‰˜ç®¡ | éƒ¨åˆ†è‡ªå·±ç»´æŠ¤ |
| **æ‰©å±•æ€§** | æœ‰é™ | æ— é™å¼¹æ€§ | ä¸­ç­‰ |
| **é€‚ç”¨åœºæ™¯** | ä¸ªäººå¼€å‘ã€æµ‹è¯• | ç”Ÿäº§ç¯å¢ƒã€å›¢é˜Ÿä½¿ç”¨ | ä¸­å°ä¼ä¸šã€æ··åˆéœ€æ±‚ |

### 2. æœ¬æœºéƒ¨ç½²ä¼˜ç¼ºç‚¹
#### ä¼˜ç‚¹
1. **æˆæœ¬æ§åˆ¶**ï¼šç¡¬ä»¶ä¸€æ¬¡æ€§æŠ•å…¥ï¼Œæ— æŒç»­è´¹ç”¨
2. **æ•°æ®å®‰å…¨**ï¼šæ•°æ®å®Œå…¨æ§åˆ¶ï¼Œä¸ç¦»å¼€æœ¬åœ°
3. **ç½‘ç»œå»¶è¿Ÿ**ï¼šå†…ç½‘è®¿é—®ï¼Œå»¶è¿Ÿæä½
4. **å®šåˆ¶çµæ´»**ï¼šå®Œå…¨è‡ªç”±é…ç½®ç¡¬ä»¶å’Œè½¯ä»¶
5. **éšç§ä¿æŠ¤**ï¼šæ— éœ€æ‹…å¿ƒç¬¬ä¸‰æ–¹æ•°æ®è®¿é—®

#### ç¼ºç‚¹
1. **ç¡¬ä»¶é™åˆ¶**ï¼šå—é™äºæœ¬åœ°ç¡¬ä»¶æ€§èƒ½
2. **ç»´æŠ¤æˆæœ¬**ï¼šéœ€è¦è‡ªå·±è´Ÿè´£è¿ç»´
3. **ç”µåŠ›æˆæœ¬**ï¼š24å°æ—¶è¿è¡Œç”µè´¹è¾ƒé«˜
4. **ç½‘ç»œè®¿é—®**ï¼šå¤–ç½‘è®¿é—®éœ€è¦é¢å¤–é…ç½®
5. **å¯é æ€§**ï¼šä¾èµ–æœ¬åœ°ç”µåŠ›ç½‘ç»œç¨³å®šæ€§

### 3. äº‘æœåŠ¡å™¨éƒ¨ç½²ä¼˜ç¼ºç‚¹
#### ä¼˜ç‚¹
1. **å¼¹æ€§ä¼¸ç¼©**ï¼šæŒ‰éœ€è°ƒæ•´CPUã€å†…å­˜ã€å­˜å‚¨
2. **é«˜å¯ç”¨æ€§**ï¼šäº‘æœåŠ¡å•†æä¾›å†—ä½™å¤‡ä»½
3. **å…¨çƒè®¿é—®**ï¼šæ”¯æŒå¤šåœ°åŸŸéƒ¨ç½²
4. **ä¸“ä¸šç»´æŠ¤**ï¼šäº‘æœåŠ¡å•†æä¾›ä¸“ä¸šè¿ç»´
5. **æˆæœ¬çµæ´»**ï¼šæŒ‰ä½¿ç”¨é‡ä»˜è´¹

#### ç¼ºç‚¹
1. **æŒç»­æˆæœ¬**ï¼šæ¯æœˆæŒç»­è´¹ç”¨
2. **æ•°æ®å®‰å…¨**ï¼šä¾èµ–äº‘æœåŠ¡å•†å®‰å…¨æªæ–½
3. **ç½‘ç»œå»¶è¿Ÿ**ï¼šå…¬ç½‘è®¿é—®å¯èƒ½æœ‰å»¶è¿Ÿ
4. **ä¾›åº”å•†é”å®š**ï¼šè¿ç§»æˆæœ¬è¾ƒé«˜
5. **é…ç½®é™åˆ¶**ï¼šå—é™äºäº‘æœåŠ¡å•†æä¾›çš„é…ç½®

### 4. éƒ¨ç½²å»ºè®®
#### ä¸ªäººå¼€å‘è€…/å­¦ä¹ è€…
- **æ¨è**ï¼šæœ¬æœºéƒ¨ç½²
- **é…ç½®**ï¼š16GBå†…å­˜ + 8GBæ˜¾å­˜çš„GPU
- **æ¨¡å‹**ï¼š7B-13Bå‚æ•°çš„é‡åŒ–æ¨¡å‹
- **æˆæœ¬**ï¼šÂ¥5000-Â¥10000ç¡¬ä»¶æŠ•å…¥

#### ä¸­å°å›¢é˜Ÿ
- **æ¨è**ï¼šæ··åˆéƒ¨ç½²
- **é…ç½®**ï¼šäº‘æœåŠ¡å™¨ + æœ¬åœ°å­˜å‚¨
- **æ¨¡å‹**ï¼š13B-70Bå‚æ•°çš„é‡åŒ–æ¨¡å‹
- **æˆæœ¬**ï¼šÂ¥1000-Â¥3000/æœˆ

#### ä¼ä¸šç”Ÿäº§ç¯å¢ƒ
- **æ¨è**ï¼šäº‘æœåŠ¡å™¨é›†ç¾¤
- **é…ç½®**ï¼šå¤šGPUèŠ‚ç‚¹ + è´Ÿè½½å‡è¡¡
- **æ¨¡å‹**ï¼š70B+å‚æ•°çš„å®Œæ•´æ¨¡å‹
- **æˆæœ¬**ï¼šÂ¥5000+/æœˆ

### 5. å…·ä½“éƒ¨ç½²æ–¹æ¡ˆ

#### æ–¹æ¡ˆAï¼šä½æˆæœ¬ä¸ªäººéƒ¨ç½²
```bash
# ç¡¬ä»¶è¦æ±‚
- CPUï¼šIntel i5/i7 10ä»£ä»¥ä¸Š æˆ– AMD Ryzen 5/7
- å†…å­˜ï¼š32GB DDR4
- æ˜¾å¡ï¼šNVIDIA RTX 3060 12GBï¼ˆæœ€ä½è¦æ±‚ï¼‰
- å­˜å‚¨ï¼š1TB NVMe SSD

# è½¯ä»¶é…ç½®
- ç³»ç»Ÿï¼šUbuntu 22.04 LTS
- Dockerï¼šæœ€æ–°ç¨³å®šç‰ˆ
- æ¨¡å‹ï¼šLlama2-7B Q4é‡åŒ–ç‰ˆ
```
#### æ–¹æ¡ˆBï¼šä¸­ç­‰è§„æ¨¡å›¢é˜Ÿéƒ¨ç½²
```yaml
# äº‘æœåŠ¡å™¨é…ç½®ï¼ˆä»¥é˜¿é‡Œäº‘ä¸ºä¾‹ï¼‰
- å®ä¾‹ï¼šecs.gn6i-c8g1.2xlarge
- CPUï¼š8æ ¸
- å†…å­˜ï¼š32GB
- GPUï¼šNVIDIA T4 16GB
- å­˜å‚¨ï¼š500GB ESSD
- å¸¦å®½ï¼š5Mbps

# æœˆæˆæœ¬ï¼šçº¦Â¥1500-Â¥2000
```
#### æ–¹æ¡ˆCï¼šé«˜æ€§èƒ½ç”Ÿäº§éƒ¨ç½²
```yaml
# Kubernetesé›†ç¾¤é…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: ollama-config
data:
  OLLAMA_NUM_GPU: "2"
  OLLAMA_MAX_MODELS: "5"

# GPUèŠ‚ç‚¹
- èŠ‚ç‚¹ç±»å‹ï¼šGPUè®¡ç®—å‹å®ä¾‹
- æ¯ä¸ªèŠ‚ç‚¹ï¼š2Ã— NVIDIA A10 24GB
- èŠ‚ç‚¹æ•°é‡ï¼š2-4ä¸ª
- å­˜å‚¨ï¼šåˆ†å¸ƒå¼å­˜å‚¨

# æœˆæˆæœ¬ï¼šÂ¥5000+
```
## ğŸ’» ç¡¬ä»¶ä¸æ“ä½œç³»ç»Ÿé€‰æ‹©æŒ‡å—

<div style="background-color: #e1f5fe; padding: 15px; border-radius: 5px; border-left: 4px solid #03A9F4;">
<strong>ğŸ’™ ç¡¬ä»¶ä¸ç³»ç»Ÿé€‰æ‹©</strong> - ä¸åŒé…ç½®å¯¹æ¯”ä¸å»ºè®®
</div>

### 1. ç¡¬ä»¶é€‰æ‹©æŒ‡å—

#### GPUé…ç½®å¯¹æ¯”è¡¨

| GPUå‹å· | æ˜¾å­˜ | è®¡ç®—èƒ½åŠ› | åŠŸè€— | ä»·æ ¼èŒƒå›´ | é€‚ç”¨åœºæ™¯ |
|---------|------|----------|------|----------|----------|
| **RTX 3060** | 12GB | ä¸­ç­‰ | 170W | Â¥2000-Â¥2500 | ä¸ªäººå­¦ä¹ ã€å°æ¨¡å‹ |
| **RTX 4060 Ti** | 16GB | ä¸­ç­‰ | 160W | Â¥3000-Â¥3500 | ä¸ªäººå¼€å‘ã€13Bæ¨¡å‹ |
| **RTX 4070** | 12GB | è‰¯å¥½ | 200W | Â¥4000-Â¥4500 | ä¸­å°å›¢é˜Ÿã€å¿«é€Ÿæ¨ç† |
| **RTX 4080** | 16GB | ä¼˜ç§€ | 320W | Â¥7000-Â¥8000 | å›¢é˜Ÿå¼€å‘ã€70Bé‡åŒ– |
| **RTX 4090** | 24GB | å“è¶Š | 450W | Â¥13000-Â¥15000 | ä¸“ä¸šå¼€å‘ã€å¤§æ¨¡å‹ |
| **NVIDIA T4** | 16GB | è‰¯å¥½ | 70W | äº‘æœåŠ¡å™¨ä¸“ç”¨ | äº‘éƒ¨ç½²ã€æ¨ç†æœåŠ¡ |
| **NVIDIA A10** | 24GB | ä¼˜ç§€ | 150W | äº‘æœåŠ¡å™¨ä¸“ç”¨ | ä¼ä¸šç”Ÿäº§ç¯å¢ƒ |

#### CPUä¸å†…å­˜é…ç½®
```yaml
# é…ç½®çº§åˆ«å¯¹æ¯”
ä½é…ç½®ï¼ˆå…¥é—¨ï¼‰:
  CPU: Intel i5 / AMD Ryzen 5 (6æ ¸12çº¿ç¨‹)
  å†…å­˜: 32GB DDR4
  å­˜å‚¨: 512GB NVMe SSD
  
ä¸­é…ç½®ï¼ˆæ¨èï¼‰:
  CPU: Intel i7 / AMD Ryzen 7 (8æ ¸16çº¿ç¨‹)
  å†…å­˜: 64GB DDR4
  å­˜å‚¨: 1TB NVMe SSD
  
é«˜é…ç½®ï¼ˆä¸“ä¸šï¼‰:
  CPU: Intel i9 / AMD Ryzen 9 (12æ ¸24çº¿ç¨‹)
  å†…å­˜: 128GB DDR4/DDR5
  å­˜å‚¨: 2TB NVMe SSD
  ```
  ### 2. æ“ä½œç³»ç»Ÿé€‰æ‹©

#### Windowsç³»ç»Ÿ
**ä¼˜ç‚¹**ï¼š
- ç”¨æˆ·å‹å¥½ï¼Œå›¾å½¢ç•Œé¢å®Œå–„
- è½¯ä»¶ç”Ÿæ€ä¸°å¯Œ
- é€‚åˆæ¡Œé¢ä½¿ç”¨

**ç¼ºç‚¹**ï¼š
- WSL2ç½‘ç»œå’ŒGPUæ”¯æŒæœ‰é™
- èµ„æºå ç”¨è¾ƒé«˜
- å‘½ä»¤è¡Œå·¥å…·ä¸å¦‚Linuxå®Œå–„

**æ¨èä½¿ç”¨åœºæ™¯**ï¼š
- ä¸ªäººå¼€å‘æµ‹è¯•
- å›¾å½¢ç•Œé¢é‡åº¦ç”¨æˆ·
- éœ€è¦è¿è¡ŒWindowsä¸“å±è½¯ä»¶

#### Ubuntu/Debianç³»ç»Ÿ
**ä¼˜ç‚¹**ï¼š
- ç¤¾åŒºæ”¯æŒå®Œå–„
- è½¯ä»¶åŒ…ç®¡ç†æ–¹ä¾¿
- Dockerå…¼å®¹æ€§å¥½
- AIå¼€å‘ç”Ÿæ€ä¸°å¯Œ

**ç¼ºç‚¹**ï¼š
- å­¦ä¹ æ›²çº¿è¾ƒé™¡
- æ¡Œé¢ç¯å¢ƒä¸å¦‚Windowsæˆç†Ÿ
- éœ€è¦ä¸€å®šLinuxçŸ¥è¯†

**æ¨èä½¿ç”¨åœºæ™¯**ï¼š
- æœåŠ¡å™¨éƒ¨ç½²
- ä¸“ä¸šå¼€å‘ç¯å¢ƒ
- éœ€è¦æœ€ä½³æ€§èƒ½çš„åœºæ™¯

#### macOSç³»ç»Ÿ
**ä¼˜ç‚¹**ï¼š
- UnixåŸºç¡€ï¼Œå‘½ä»¤è¡Œå‹å¥½
- ç¡¬ä»¶é›†æˆä¼˜ç§€
- å¼€å‘ä½“éªŒè‰¯å¥½

**ç¼ºç‚¹**ï¼š
- GPUæ”¯æŒæœ‰é™ï¼ˆMç³»åˆ—èŠ¯ç‰‡ï¼‰
- ä»·æ ¼æ˜‚è´µ
- Dockeræ€§èƒ½ä¸å¦‚Linux

**æ¨èä½¿ç”¨åœºæ™¯**ï¼š
- å‰ç«¯å¼€å‘
- ç§»åŠ¨å¼€å‘
- éGPUå¯†é›†å‹ä»»åŠ¡

### 3. å…·ä½“é…ç½®å»ºè®®

#### æ–¹æ¡ˆ1ï¼šä¸ªäººå­¦ä¹ é…ç½®ï¼ˆÂ¥5000-Â¥8000ï¼‰
```yaml
ç¡¬ä»¶é…ç½®:
  å¤„ç†å™¨: Intel i5-13400 / AMD Ryzen 5 7600
  å†…å­˜: 32GB DDR4 3200MHz
  æ˜¾å¡: NVIDIA RTX 3060 12GB
  å­˜å‚¨: 1TB NVMe SSD
  ç”µæº: 650W 80+ Bronze
  
è½¯ä»¶é…ç½®:
  æ“ä½œç³»ç»Ÿ: Ubuntu 22.04 LTS
  è™šæ‹ŸåŒ–: Docker 24.0+
  æ¨¡å‹: Llama2-7B / Mistral-7B
  é‡åŒ–: Q4_K_M
  ```
  #### æ–¹æ¡ˆ2ï¼šå›¢é˜Ÿå¼€å‘é…ç½®ï¼ˆÂ¥10000-Â¥15000ï¼‰
```yaml
ç¡¬ä»¶é…ç½®:
  å¤„ç†å™¨: Intel i7-13700 / AMD Ryzen 7 7700X
  å†…å­˜: 64GB DDR5 4800MHz
  æ˜¾å¡: NVIDIA RTX 4070 12GB
  å­˜å‚¨: 2TB NVMe SSD
  ç”µæº: 750W 80+ Gold
  
è½¯ä»¶é…ç½®:
  æ“ä½œç³»ç»Ÿ: Ubuntu 22.04 LTS Server
  å®¹å™¨ç¼–æ’: Docker Compose
  æ¨¡å‹: Llama2-13B / Qwen-14B
  é‡åŒ–: Q5_K_M
  ```
  #### æ–¹æ¡ˆ3ï¼šç”Ÿäº§ç¯å¢ƒé…ç½®ï¼ˆÂ¥20000+ï¼‰
```yaml
ç¡¬ä»¶é…ç½®:
  å¤„ç†å™¨: Intel i9-13900 / AMD Ryzen 9 7900X
  å†…å­˜: 128GB DDR5 5200MHz
  æ˜¾å¡: NVIDIA RTX 4090 24GB Ã— 2
  å­˜å‚¨: 4TB NVMe SSD RAID 0
  ç”µæº: 1200W 80+ Platinum
  
è½¯ä»¶é…ç½®:
  æ“ä½œç³»ç»Ÿ: Ubuntu 22.04 LTS
  å®¹å™¨ç¼–æ’: Kubernetes
  æ¨¡å‹: Llama2-70B / Mixtral-8x7B
  é‡åŒ–: Q4_0 (å†…å­˜è¶³å¤Ÿæ—¶ç”¨Q8_0)
  ```
  ### 4. äº‘æœåŠ¡å•†é€‰æ‹©

#### å›½å†…äº‘æœåŠ¡å•†å¯¹æ¯”
| æœåŠ¡å•† | GPUå®ä¾‹ | ä»·æ ¼/æœˆ | ä¼˜åŠ¿ | ç¼ºç‚¹ |
|--------|---------|---------|------|------|
| **é˜¿é‡Œäº‘** | GNç³»åˆ— | Â¥1500+ | ç”Ÿæ€å®Œå–„ï¼Œæ–‡æ¡£å…¨ | ä»·æ ¼è¾ƒé«˜ |
| **è…¾è®¯äº‘** | GPUè®¡ç®—å‹ | Â¥1200+ | ä»·æ ¼é€‚ä¸­ï¼Œæ´»åŠ¨å¤š | ç½‘ç»œè´¨é‡ä¸ç¨³å®š |
| **åä¸ºäº‘** | Gç³»åˆ— | Â¥1300+ | å®‰å…¨æ€§å¥½ï¼Œæ”¿ä¼å®¢æˆ·å¤š | ç”Ÿæ€ç›¸å¯¹å°é—­ |
| **ç™¾åº¦äº‘** | GPUåŠ é€Ÿå‹ | Â¥1100+ | AIç”Ÿæ€å®Œå–„ | å¸‚åœºä»½é¢è¾ƒå° |

#### å›½é™…äº‘æœåŠ¡å•†å¯¹æ¯”
| æœåŠ¡å•† | GPUå®ä¾‹ | ä»·æ ¼/æœˆ | ä¼˜åŠ¿ | ç¼ºç‚¹ |
|--------|---------|---------|------|------|
| **AWS** | G4/G5 | $200+ | å…¨çƒè¦†ç›–ï¼ŒæœåŠ¡ç¨³å®š | ä»·æ ¼è¾ƒé«˜ï¼Œé…ç½®å¤æ‚ |
| **Google Cloud** | A2/T4 | $180+ | TPUæ”¯æŒï¼ŒAIç”Ÿæ€å¥½ | ä¸­å›½è®¿é—®å›°éš¾ |
| **Azure** | NCç³»åˆ— | $190+ | ä¼ä¸šé›†æˆå¥½ | å­¦ä¹ æ›²çº¿é™¡ |

### 5. æˆæœ¬ä¼˜åŒ–å»ºè®®

#### ç¡¬ä»¶é‡‡è´­ä¼˜åŒ–
```bash
# 1. äºŒæ‰‹ç¡¬ä»¶è€ƒè™‘
- è€ƒè™‘äºŒæ‰‹RTX 3090 24GBï¼ˆæ€§ä»·æ¯”é«˜ï¼‰
- æœåŠ¡å™¨å†…å­˜ï¼ˆECCå†…å­˜æ›´ç¨³å®šï¼‰
- ä¼ä¸šçº§SSDï¼ˆå¯¿å‘½æ›´é•¿ï¼‰

# 2. åˆ†æœŸå‡çº§
- å…ˆè´­ä¹°åŸºç¡€é…ç½®
- æ ¹æ®éœ€æ±‚é€æ­¥å‡çº§
- ä¿ç•™å‡çº§ç©ºé—´

# 3. ç”µåŠ›æˆæœ¬è®¡ç®—
æ€»åŠŸè€— = (CPUåŠŸè€— + GPUåŠŸè€— Ã— GPUæ•°é‡ + å…¶ä»–) Ã— 24å°æ—¶ Ã— 30å¤© Ã· 1000
æœˆç”µè´¹ = æ€»åŠŸè€— Ã— ç”µä»·ï¼ˆå¦‚0.6å…ƒ/åº¦ï¼‰
```
#### äº‘æœåŠ¡æˆæœ¬ä¼˜åŒ–
```yaml
# 1. é¢„ç•™å®ä¾‹ï¼ˆReserved Instancesï¼‰
- è´­ä¹°1å¹´/3å¹´é¢„ç•™å®ä¾‹ï¼ŒæŠ˜æ‰£30-60%
- é€‚åˆé•¿æœŸç¨³å®šä½¿ç”¨çš„åœºæ™¯

# 2. ç«ä»·å®ä¾‹ï¼ˆSpot Instancesï¼‰
- ä»·æ ¼ä¾¿å®œ70-90%
- å¯èƒ½éšæ—¶è¢«å›æ”¶
- é€‚åˆè®­ç»ƒä»»åŠ¡ã€æ‰¹å¤„ç†

# 3. è‡ªåŠ¨ä¼¸ç¼©
- æ ¹æ®è´Ÿè½½è‡ªåŠ¨è°ƒæ•´å®ä¾‹æ•°é‡
- éé«˜å³°æ—¶æ®µå‡å°‘å®ä¾‹
- è®¾ç½®åˆç†çš„ä¼¸ç¼©ç­–ç•¥
```
## âš¡ æ€§èƒ½ä¼˜åŒ–ä¸è°ƒä¼˜å»ºè®®

<div style="background-color: #e8f5e8; padding: 15px; border-radius: 5px; border-left: 4px solid #4CAF50;">
<strong>ğŸ’š æ€§èƒ½ä¼˜åŒ–</strong> - ç³»ç»Ÿã€æ¨¡å‹ã€æ¨ç†ä¼˜åŒ–
</div>

### 1. ç³»ç»Ÿçº§ä¼˜åŒ–

#### Linuxå†…æ ¸ä¼˜åŒ–
```bash
# ç¼–è¾‘ /etc/sysctl.conf æ·»åŠ ä»¥ä¸‹é…ç½®
# ç½‘ç»œä¼˜åŒ–
net.core.rmem_max = 134217728
net.core.wmem_max = 134217728
net.ipv4.tcp_rmem = 4096 87380 134217728
net.ipv4.tcp_wmem = 4096 65536 134217728
net.core.netdev_max_backlog = 10000
net.core.somaxconn = 4096

# å†…å­˜ä¼˜åŒ–
vm.swappiness = 10
vm.dirty_ratio = 60
vm.dirty_background_ratio = 2
vm.vfs_cache_pressure = 50

# æ–‡ä»¶ç³»ç»Ÿä¼˜åŒ–
fs.file-max = 2097152
fs.inotify.max_user_watches = 524288

# åº”ç”¨é…ç½®
sudo sysctl -p
```
#### Dockeræ€§èƒ½ä¼˜åŒ–
```bash
# 1. ä½¿ç”¨æ€§èƒ½æ›´å¥½çš„å­˜å‚¨é©±åŠ¨
# ç¼–è¾‘ /etc/docker/daemon.json
{
  "storage-driver": "overlay2",
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "10m",
    "max-file": "3"
  },
  "live-restore": true,
  "default-ulimits": {
    "nofile": {
      "Name": "nofile",
      "Soft": 65536,
      "Hard": 65536
    }
  }
}

# 2. ä½¿ç”¨tmpfsæŒ‚è½½ä¸´æ—¶æ–‡ä»¶
docker run -d \
  --tmpfs /tmp \
  --tmpfs /run \
  -p 3000:8080 \
  open-webui

# 3. æ¸…ç†æ— ç”¨çš„Dockerèµ„æº
docker system prune -a --volumes
docker image prune
docker container prune
docker network prune
```
### 2. æ¨¡å‹çº§ä¼˜åŒ–

#### é‡åŒ–ç­–ç•¥ä¼˜åŒ–
```python
# Pythonä»£ç ç¤ºä¾‹ï¼šé‡åŒ–æ¨¡å‹é€‰æ‹©ç­–ç•¥
def select_quantization_level(memory_gb, gpu_memory_gb, model_size_gb):
    """
    æ ¹æ®å¯ç”¨å†…å­˜é€‰æ‹©æœ€ä½³é‡åŒ–çº§åˆ«
    """
    total_memory = memory_gb + gpu_memory_gb
    
    if total_memory >= 32 and gpu_memory_gb >= 24:
        return "Q8_0"   # æœ€é«˜è´¨é‡ï¼Œéœ€è¦å¤§å†…å­˜
    elif total_memory >= 24 and gpu_memory_gb >= 16:
        return "Q6_K"   # é«˜è´¨é‡
    elif total_memory >= 16:
        return "Q5_K_M" # å¹³è¡¡è´¨é‡å’Œæ€§èƒ½
    elif total_memory >= 8:
        return "Q4_K_M" # è¾ƒå¥½çš„æ€§èƒ½
    else:
        return "Q4_0"   # æœ€å°å†…å­˜å ç”¨
```        
#### æ¨¡å‹ç¼“å­˜ä¼˜åŒ–
```bash
# 1. é¢„åŠ è½½å¸¸ç”¨æ¨¡å‹
# åˆ›å»ºå¯åŠ¨è„šæœ¬ preload_models.sh
#!/bin/bash
models=("llama2:7b" "mistral:7b" "qwen2.5:7b")

for model in "${models[@]}"; do
    ollama pull "$model"
    echo "å·²åŠ è½½æ¨¡å‹: $model"
done

# 2. é…ç½®æ¨¡å‹ç¼“å­˜
export OLLAMA_KEEP_ALIVE=300  # ä¿æŒæ¨¡å‹åŠ è½½300ç§’
export OLLAMA_MAX_LOADED_MODELS=3  # æœ€å¤šåŒæ—¶åŠ è½½3ä¸ªæ¨¡å‹

# 3. ç›‘æ§æ¨¡å‹å†…å­˜ä½¿ç”¨
watch -n 5 "ollama ps"
```
### 3. æ¨ç†ä¼˜åŒ–

#### æ‰¹å¤„ç†ä¼˜åŒ–
```python
# æ‰¹é‡æ¨ç†ç¤ºä¾‹
import asyncio
from typing import List

async def batch_inference(prompts: List[str], batch_size: int = 4):
    """
    æ‰¹é‡æ¨ç†ä¼˜åŒ–
    """
    results = []
    
    for i in range(0, len(prompts), batch_size):
        batch = prompts[i:i+batch_size]
        batch_results = await asyncio.gather(*[
            single_inference(prompt) for prompt in batch
        ])
        results.extend(batch_results)
    
    return results

# å¼‚æ­¥å•æ¬¡æ¨ç†
async def single_inference(prompt: str):
    import aiohttp
    import json
    
    url = "http://localhost:11434/api/generate"
    data = {
        "model": "llama2",
        "prompt": prompt,
        "stream": False
    }
    
    async with aiohttp.ClientSession() as session:
        async with session.post(url, json=data) as response:
            result = await response.json()
            return result.get("response", "")
```
#### æ¨ç†å‚æ•°è°ƒä¼˜
```python
# åŠ¨æ€å‚æ•°è°ƒæ•´
def optimize_inference_parameters(context_length: int, task_type: str):
    """
    æ ¹æ®ä¸Šä¸‹æ–‡é•¿åº¦å’Œä»»åŠ¡ç±»å‹ä¼˜åŒ–æ¨ç†å‚æ•°
    """
    base_params = {
        "temperature": 0.7,
        "top_p": 0.9,
        "top_k": 40,
        "repeat_penalty": 1.1,
        "num_predict": 512
    }
    
    if task_type == "creative":
        base_params["temperature"] = 0.9
        base_params["top_p"] = 0.95
    elif task_type == "technical":
        base_params["temperature"] = 0.3
        base_params["top_p"] = 0.7
    
    if context_length > 2048:
        base_params["num_predict"] = 256  # é•¿ä¸Šä¸‹æ–‡å‡å°‘è¾“å‡ºé•¿åº¦
    
    return base_params

# GPUå†…å­˜ä¼˜åŒ–å‚æ•°
def gpu_memory_optimization(gpu_memory_gb: int, model_size_gb: int):
    """
    GPUå†…å­˜ä¼˜åŒ–é…ç½®
    """
    config = {
        "gpu_layers": 0,
        "main_gpu": 0,
        "tensor_split": []
    }
    
    available_gpu_memory = gpu_memory_gb * 1024  # è½¬æ¢ä¸ºMB
    
    if available_gpu_memory >= model_size_gb * 1024:
        # GPUå†…å­˜è¶³å¤Ÿï¼Œå®Œå…¨åŠ è½½åˆ°GPU
        config["gpu_layers"] = -1  # å…¨éƒ¨å±‚åŠ è½½åˆ°GPU
    else:
        # GPUå†…å­˜ä¸è¶³ï¼Œéƒ¨åˆ†åŠ è½½
        layers_to_gpu = int((available_gpu_memory / (model_size_gb * 1024)) * 100)
        config["gpu_layers"] = max(20, layers_to_gpu)  # è‡³å°‘20å±‚
    
    return config
```
### 4. ç½‘ç»œä¼˜åŒ–

#### è´Ÿè½½å‡è¡¡é…ç½®
```yaml
# Nginxè´Ÿè½½å‡è¡¡é…ç½®
upstream ollama_cluster {
    least_conn;  # æœ€å°‘è¿æ¥æ•°ç®—æ³•
    server 192.168.1.10:11434 max_fails=3 fail_timeout=30s;
    server 192.168.1.11:11434 max_fails=3 fail_timeout=30s;
    server 192.168.1.12:11434 max_fails=3 fail_timeout=30s;
    
    keepalive 32;
}

server {
    listen 80;
    server_name ollama.yourdomain.com;
    
    location / {
        proxy_pass http://ollama_cluster;
        proxy_http_version 1.1;
        proxy_set_header Connection "";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        
        # è¶…æ—¶è®¾ç½®
        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
        proxy_read_timeout 300s;
        
        # ç¼“å†²è®¾ç½®
        proxy_buffering on;
        proxy_buffer_size 4k;
        proxy_buffers 8 4k;
        proxy_busy_buffers_size 8k;
    }
}
```
#### WebSocketè¿æ¥ä¼˜åŒ–
```nginx
# WebSocketé•¿è¿æ¥ä¼˜åŒ–
location /api/generate {
    proxy_pass http://ollama_cluster;
    proxy_http_version 1.1;
    proxy_set_header Upgrade $http_upgrade;
    proxy_set_header Connection "upgrade";
    proxy_set_header Host $host;
    
    # é•¿è¿æ¥è¶…æ—¶
    proxy_read_timeout 86400s;  # 24å°æ—¶
    proxy_send_timeout 86400s;
    
    # ç¼“å†²åŒºä¼˜åŒ–
    proxy_buffering off;
    proxy_buffer_size 16k;
    proxy_buffers 4 32k;
    
    # å¿ƒè·³æ£€æµ‹
    proxy_connect_timeout 30s;
}

# é™åˆ¶å•ä¸ªIPè¿æ¥æ•°
limit_conn_zone $binary_remote_addr zone=ollama_limit:10m;

location /api/ {
    limit_conn ollama_limit 10;  # æ¯ä¸ªIPæœ€å¤š10ä¸ªè¿æ¥
    limit_rate 1m;  # é™åˆ¶å¸¦å®½1MB/s
    
    proxy_pass http://ollama_cluster;
}
```
### 5. ç›‘æ§ä¸å‘Šè­¦

#### Prometheusç›‘æ§é…ç½®
```yaml
# prometheus.ymlé…ç½®
scrape_configs:
  - job_name: 'ollama'
    static_configs:
      - targets: ['192.168.1.10:11434', '192.168.1.11:11434']
    metrics_path: '/api/metrics'
    scrape_interval: 30s
    scrape_timeout: 10s
    
  - job_name: 'open-webui'
    static_configs:
      - targets: ['192.168.1.20:8080']
    metrics_path: '/api/v1/metrics'
    scrape_interval: 30s

# å‘Šè­¦è§„åˆ™
rule_files:
  - "alerts.yml"
```
#### Grafanaä»ªè¡¨æ¿é…ç½®
```json
{
  "dashboard": {
    "title": "å¤§æ¨¡å‹æœåŠ¡ç›‘æ§",
    "panels": [
      {
        "title": "GPUä½¿ç”¨ç‡",
        "targets": [
          {
            "expr": "100 * (nvidia_gpu_utilization / 100)",
            "legendFormat": "GPU {{gpu}}"
          }
        ],
        "type": "graph",
        "yaxes": [
          {"format": "percent", "min": 0, "max": 100}
        ]
      },
      {
        "title": "æ¨¡å‹å†…å­˜ä½¿ç”¨",
        "targets": [
          {
            "expr": "ollama_model_memory_bytes / 1024 / 1024 / 1024",
            "legendFormat": "{{model}}"
          }
        ],
        "type": "graph",
        "yaxes": [
          {"format": "bytes", "min": 0}
        ]
      },
      {
        "title": "è¯·æ±‚å“åº”æ—¶é—´",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, sum(rate(ollama_request_duration_seconds_bucket[5m])) by (le))",
            "legendFormat": "P95å“åº”æ—¶é—´"
          }
        ],
        "type": "graph",
        "yaxes": [
          {"format": "s", "min": 0}
        ]
      }
    ]
  }
}
```
#### å‘Šè­¦è§„åˆ™é…ç½®
```yaml
# alerts.yml
groups:
  - name: ollama_alerts
    rules:
      - alert: HighGPUUsage
        expr: nvidia_gpu_utilization > 90
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "GPUä½¿ç”¨ç‡è¿‡é«˜"
          description: "GPU {{ $labels.gpu }} ä½¿ç”¨ç‡å·²è¾¾ {{ $value }}%"
          
      - alert: ModelMemoryHigh
        expr: ollama_model_memory_bytes / 1024 / 1024 / 1024 > 16
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "æ¨¡å‹å†…å­˜ä½¿ç”¨è¿‡é«˜"
          description: "æ¨¡å‹ {{ $labels.model }} å†…å­˜ä½¿ç”¨è¶…è¿‡16GB"
          
      - alert: HighRequestLatency
        expr: histogram_quantile(0.95, rate(ollama_request_duration_seconds_bucket[5m])) > 10
        for: 3m
        labels:
          severity: warning
        annotations:
          summary: "è¯·æ±‚å“åº”æ—¶é—´è¿‡é•¿"
          description: "P95å“åº”æ—¶é—´è¶…è¿‡10ç§’"
          
      - alert: ServiceDown
        expr: up{job="ollama"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "OllamaæœåŠ¡å®•æœº"
          description: "Ollamaå®ä¾‹ {{ $labels.instance }} å·²å®•æœº"
```

